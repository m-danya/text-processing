### Дескрипшн

- Использовал модель https://huggingface.co/cointegrated/rubert-tiny2 и её токенизатор.
- Использовал BIO-разметку
- Тренировал на GPU, потом перебросил модель на CPU перед сохранением, т.к. иначе
модель никак не получится загрузить! https://github.com/huggingface/transformers/issues/259
- Обучал 50 эпох, но лосс на валидации начинает расти уже после 4 эпохи!
- Беру тексты из файлов целиком. Один из них преобразуется в > 2048 токенов, этот пример
выкинул из обучения. В dev и test таких примеров нет
- Парсил спаны и оффсеты от токенайзера: сопоставлял как можно более аккуратно
(убил кучу времени на дебаг).
- Сущности, которые являются внешними по отношению к другим, ВЫКИДЫВАЛ
- Веса предательски не влезали в 100 Мебибайт (зароляло в итоге, что ограничение
не в Мегабайтах), поэтому пытался сжать веса отдельно. Пробовал через zipfile со
степенью сжатия zipfile.ZIP_LZMA, но этого не хватало. В итоге воспользовался гиперкрутым
сжатием с помощью библиотеки lzma, где можно параметры сжатия менять, в отличие от zipfile
- Как ограничить число тредов в transformers, я не нашёл
- Классы датасетов получил как гибрид датасета из задания по предмету "DL" и кастомного
датасета из документации
- Код train.py копировал из ноутбука, поэтому кодстайл такой странный
- Метрики мне стало лень писать, решил сразу заливать в систему


### Воспроизведение
- иметь библиотеки transformers и torch
- датасет положить в папку `nerel` в корень этой папки (./nerel/train, ./nerel/dev, ./nerel/test)
- если на машине нет GPU, убрать из train.py строчку "model.cuda()"
- запустить train.py
- теперь можно тестить solution.py

## Полученный скор

Baseline: 0.3627272207
Полученный скор: 0.5528846474
